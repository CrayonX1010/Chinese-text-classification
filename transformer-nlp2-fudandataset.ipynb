{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a5daf9b-786d-4d02-8360-6f5007723411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "类别: C11-Space, 文章数: 30\n",
      "类别: C15-Energy, 文章数: 30\n",
      "类别: C16-Electronics, 文章数: 27\n",
      "类别: C17-Communication, 文章数: 25\n",
      "类别: C19-Computer, 文章数: 30\n",
      "类别: C23-Mine, 文章数: 30\n",
      "类别: C29-Transport, 文章数: 30\n",
      "类别: C3-Art, 文章数: 30\n",
      "类别: C31-Enviornment, 文章数: 30\n",
      "类别: C32-Agriculture, 文章数: 30\n",
      "类别: C34-Economy, 文章数: 30\n",
      "类别: C35-Law, 文章数: 30\n",
      "类别: C36-Medical, 文章数: 30\n",
      "类别: C37-Military, 文章数: 30\n",
      "类别: C38-Politics, 文章数: 30\n",
      "类别: C39-Sports, 文章数: 30\n",
      "类别: C4-Literature, 文章数: 30\n",
      "类别: C5-Education, 文章数: 30\n",
      "类别: C6-Philosophy, 文章数: 30\n",
      "类别: C7-History, 文章数: 30\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "def load_data_from_directory(data_dir, num_files=30):\n",
    "    \"\"\"\n",
    "    从每个类别文件夹中加载前`num_files`篇文章。\n",
    "    \"\"\"\n",
    "    category_files = {}\n",
    "    \n",
    "    # 遍历每个类别文件夹\n",
    "    for category in os.listdir(data_dir):\n",
    "        category_path = os.path.join(data_dir, category)\n",
    "        \n",
    "        if os.path.isdir(category_path):  # 确保是文件夹\n",
    "            # 获取该类别文件夹下的所有txt文件\n",
    "            files = [f for f in os.listdir(category_path) if f.endswith('.txt')]\n",
    "            \n",
    "            # 随机选择前num_files篇文章\n",
    "            selected_files = files[:num_files]  # 选择前30篇文件\n",
    "            \n",
    "            category_files[category] = []\n",
    "            for file in selected_files:\n",
    "                file_path = os.path.join(category_path, file)\n",
    "                with open(file_path, 'r', encoding='gbk',errors='ignore') as f:\n",
    "                    content = f.read()\n",
    "                category_files[category].append(content)\n",
    "    \n",
    "    return category_files\n",
    "\n",
    "# 使用函数加载数据\n",
    "train_data_dir = \"D:\\\\nlp文档\\\\fudan\\\\train\\\\train\"  # 替换为你的训练集路径\n",
    "train_data = load_data_from_directory(train_data_dir, num_files=30)\n",
    "\n",
    "# 打印每个类别的前30篇文章\n",
    "for category, texts in train_data.items():\n",
    "    print(f\"类别: {category}, 文章数: {len(texts)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ade81dcb-65fa-4b0f-8328-90804b9280ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\YUXIYU~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.936 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['【', ' ', '文献号', ' ', '】', '1', '-', '2340', '\\n', '【', '原文', '出处', '】', '中国', '图书', '评论', '\\n', '【', '原刊', '地名', '】', '沈阳', '\\n', '【', '原刊', '期号', '】', '199510', '\\n', '【', '原刊', '页', '号', '】', '61', '-', '62', '\\n', '【', '分', ' ', '类', ' ', '号', '】', 'Z1', '\\n', '【', '分', ' ', '类', ' ', '名', '】', '出版', '工作', '、', '图书', '评介', '\\n', '【', ' ', '作', ' ', ' ', '者', ' ', '】', '杨', '小民', '\\n', '【', '复印', '期号', '】', '199602', '\\n', '【', ' ', '标', ' ', ' ', '题', ' ', '】', '图书', '评论', '应当', '重视', '对', '书籍装帧', '艺术', '的', '评价', '\\n', '【', ' ', '正', ' ', ' ', '文', ' ', '】', '\\n', ' ', ' ', ' ', ' ', '图书', '评论', '是', '近代', '报刊', '业', '兴起', '后', '，', '在', '世界', '各国', '得到', '长足发展', '的', '一种', '新型', '评论', '体裁', '。', '而', '不论是', '书评', '理论', '还是', '书评', '实践', '都', '有', '一个', '不小', '的', '疏漏', '，', '即', '忽', '\\n', '视', '了', '图书', '的', '形式', '因素', '。', '因为', '图书', '是', '内容', '与', '形式', '的', '综合体', '，', '忽视', '了', '“', '图书', '形式', '”', '这一', '重要', '方面', '，', '会', '导致', '在', '图书', '评论', '活动', '中', '忽视', '对', '图书', '的', '出版', '形式', '这', '\\n', '一', '重要', '方面', '的', '品评', '论述', '，', '而', '这', '对于', '出版物', '的', '达到', '基本', '要求', '：', '“', '形神', '俱佳', '”', '（', '“', '形', '”', '指书装', '艺术', '，', '“', '神', '”', '指', '内容', '叙述', '）', '或', '最高', '要求', '“', '尽善尽美', '”', '（', '\\n', '“', '尽善', '”', '指', '内容', '而言', '，', '“', '尽美', '”', '指', '形式', '而言', '）', '无疑', '是', '有', '缺憾', '的', '。', '\\n', ' ', ' ', ' ', ' ', '图书', '的', '形式', '因素', '即', '为', '书籍', '的', '装帧', '设计', '艺术', '（', '以下', '简称', '“', '书装', '艺术', '”', '）', '。', '它', '的', '内容', '应当', '包括', '：', '封面', '、', '封底', '、', '书脊', '、', '环衬', '、', '扉页', '、', '字体', '、', '字号', '、', '\\n', '插图', '、', '版式', '、', '护封', '等', '。', '装帧', '设计', '应是', '图书', '中', '的', '重要', '内容', '，', '顺理成章', '地应', '成为', '书评', '文章', '中', '不可或缺', '的', '评论', '对象', '。', '然而', '，', '在', '当前', '报刊', '上', '大量', '刊登', '的', '书', '\\n', '评', '文章', '中', '谈及', '这', '一方面', '的', '极为', '少见', '。', '这一', '偏颇', '势必会', '对', '中国', '出版物', '综合', '水平', '的', '提高', '产生', '不良', '的', '影响', '。', '\\n', ' ', ' ', ' ', ' ', '图书', '出版事业', '是', '人类', '的', '思维', '活动', '和', '精神', '成果', '与', '科学', '技术相结合', '的', '一项', '系统工程', '。', '而书装', '艺术', '则', '渗透', '着', '“', '出版', '人', '”', '的', '思维', '活动', '和', '印刷', '科技', '的', '水', '\\n', '平', '两个', '因素', '。', '设计者', '的', '艺术', '构思', '，', '通过', '印刷', '工艺', '的', '精心制作', '，', '与', '图书', '的', '内容', '达到', '协调一致', '，', '才', '形成', '一本', '精美', '的', '形神', '俱佳', '的', '图书', '。', '\\n', ' ', ' ', ' ', ' ', '如今', '，', '我国', '的', '一些', '出版社', '，', '对', '图书', '的', '装帧', '设计', '重视', '不够', '，', '这', '既', '成为', '书评', '作者', '忽视', '书装', '艺术', '的', '评论', '的', '一个', '潜因', '，', '他们', '认为', '许多', '图书', '的', '书装', '艺术', '\\n', '不值一提', '或', '难以', '一说', '；', '同时', '，', '也', '人为', '地', '造成', '了', '对', '书装', '艺术', '粗糙', '现象', '的', '不合理', '宽容', '。', '究其原因', '，', '出版社', '不愿', '投入', '应有', '的', '资金', '和', '人力', '是', '主要', '问题', '。', '书', '\\n', '装', '艺术', '本身', '也', '是', '体现', '出版物', '品位', '高低', '的', '一项', '重要', '因素', '。', '在', '现代', '图书', '出版', '印刷', '中', '，', '应', '投入', '必要', '的', '资金', '，', '以', '避免', '参加', '国际', '图书', '博览会', '的', '中国', '图书', '再', '被', '\\n', '人们', '讥笑', '为', '“', '展翅高飞', '”', '、', '“', '鞠躬尽瘁', '”', '了', '。', '（', '由于', '纸质', '差', '，', '装订', '落后', '，', '我国', '图书', '陈列', '于', '国际', '展台', '时', '，', '暖气', '会', '使', '书册', '张开', '弯曲', '，', '这', '叫', '“', '展翅高飞', '\\n', '”', '；', '还有', '则', '为', '书脊', '软塌', '，', '不能', '直立', '，', '弯腰驼背', '，', '则', '称', '“', '鞠躬尽瘁', '”', '。', '）', '\\n', ' ', ' ', ' ', ' ', '编辑', '素养', '的', '欠缺', '，', '也', '直接', '影响', '到', '书装', '艺术', '的', '优劣', '。', '在', '我国', '的', '出版业', '中', '，', '编辑', '通常', '是', '提供', '书装', '要求', '，', '并', '参与', '设计方案', '的', '。', '参与', '的', '前提', '，', '应该', '是', '\\n', '要', '具备', '一定', '的', '艺术', '素质', '和', '审美眼光', '，', '但', '如今', '有', '相当', '一部分', '编辑', '缺乏', '这', '一点', '。', '他们', '对', '艺术', '规律', '，', '对', '美术', '设计者', '从事', '的', '工作', '特性', '知之甚少', '，', '他们', '的', '参', '\\n', '与', '从', '某种意义', '上', '来说', '甚至', '成为', '一种', '盲目', '的', '干涉', '：', '“', '外行', '”', '指挥', '“', '内行', '”', '。', '大至', '约束', '个', '框子', '，', '小至', '书名', '作者', '的', '位置', '安放', '和', '颜色', '的', '指派', '。', '不难设想', '，', '\\n', '在', '这种', '缺乏', '平等', '探讨', '的', '格局', '下', '，', '要求', '所', '设计', '出来', '的', '封扉', '等', '的', '艺术', '效果', '将', '是', '什么', '样子', '。', '\\n', ' ', ' ', ' ', ' ', '当然', '，', '提出', '这些', '问题', '，', '并', '不是', '反对', '文字编辑', '对', '美编', '工作', '的', '参与', '，', '而是', '希望', '各个', '出版社', '应', '在', '平时', '增加', '对书装', '艺术', '的', '知识', '的', '介绍', '和', '培训', '，', '以', '指导', '\\n', '编辑', '们', '以', '科学', '艺术', '的', '眼光', '来', '参与', '并', '审定', '书装', '设计方案', '，', '使', '我们', '的', '出版物', '真正', '成为', '内容', '与', '形式美', '和谐', '统一', '的', '精神', '产品', '。', '\\n', ' ', ' ', ' ', ' ', '书评', '工作者', '本身', '的', '观念', '的', '局限', '是', '导致', '书评', '活动', '中', '忽视', '对书装', '艺术', '作出评价', '的', '一个', '重要性', '因素', '。', '\\n', ' ', ' ', ' ', ' ', '书评', '不同于', '文艺', '评论', '。', '文艺', '评论', '是', '对', '文艺作品', '进行', '的', '学术界', '定', '。', '当前', '，', '书评', '文章', '中', '有种', '不良倾向', '—', '—', '书评', '朝', '文艺', '评论', '方向', '发展', '。', '这', '就', '违背', '了', '\\n', '书评', '的', '宗旨', '，', '降低', '了', '书评', '本身', '的', '价值', '。', '仅仅', '注意', '抓', '框架结构', '，', '评', '内容', '主题', '，', '而', '忽略', '了', '外', '在', '形式', '因素', '。', '这种', '评论', '方式', '是', '不', '完整', '的', '，', '也', '是', '不', '科学', '的', '。', '\\n', '所以', '，', '书评', '人员', '应', '调整', '自己', '的', '书评', '观念', '，', '把', '书', '的', '内容', '与', '形式', '因素', '放到', '同等', '重要', '的', '地位', '（', '不', '否认', '因文', '而', '有', '主次', '之分', '）', '，', '进行', '综合', '评论', '。', '唯其如此', '，', '\\n', '一篇', '完整', '而', '优秀', '的', '书评', '，', '才能', '使', '出版者', '、', '著作者', '、', '编辑者', '和', '读者', '多方面', '的', '获益', '。', '\\n', ' ', ' ', ' ', ' ', '书装', '艺术', '既然', '是', '构成', '图书', '的', '有机', '组成部分', '，', '那么', '，', '缺少', '对书装', '艺术', '的', '评价', '就', '意味着', '书评', '工作', '的', '不', '完整', '。', '\\n', ' ', ' ', ' ', ' ', '图书', '是', '精神', '和', '物质', '、', '内容', '和', '形式', '的', '综合体', '，', '是', '人类', '社会', '的', '精神', '产品', '。', '书装', '艺术', '是', '构成', '图书', '的', '重要', '组成部分', '，', '正如', '高斯', '先生', '在', '《', '出版', '审美', '论', '》', '\\n', '（', '1994', '年版', '）', '中所言', '：', '“', '图书', '的', '装帧', '设计', '，', '不仅', '为', '图书', '穿', '上', '一套', '美观', '的', '外衣', '，', '而且', '应该', '使', '图书', '的', '形式', '通过', '艺术', '构思', '、', '艺术', '手法', '而', '和', '内容', '统一', '起来', '\\n', '，', '反映', '出', '图书', '内容', '的', '美', '，', '反映', '出', '图书', '所', '蕴含', '的', '生命力', '的', '美', '。', '”', '\\n', ' ', ' ', ' ', ' ', '“', '…', '…', '一部', '图书', '的', '装帧', '设计', '，', '其', '审美', '价值', '虽然', '只', '属于', '个体', '，', '但', '个体', '的', '积累', '，', '却', '可以', '造成', '一个', '历史', '时期', '的', '出版事业', '的', '审美', '价值', '。', '”', '\\n', ' ', ' ', ' ', ' ', '这些', '论述', '足以', '说明', '，', '装帧', '设计', '对于', '图书', '，', '除了', '形式美', '方面', '有', '其', '重要', '意义', '和', '作用', '外', '，', '更', '有', '在', '提高', '图书', '整体', '质量', '上', '的', '重要', '意义', '和', '重要', '作用', '。', '\\n', ' ', ' ', ' ', ' ', '装帧', '设计', '本身', '，', '具有', '独特', '的', '艺术', '价值', '。', '同时', '，', '书装', '艺术', '也', '起', '着', '一种', '以', '艺术', '形式', '宣示', '图书', '内容', '的', '直观', '作用', '。', '图书', '进入', '流通领域', '，', '这种', '宣示', '既发', '\\n', '挥', '了', '一种', '无可替代', '的', '引导', '读者', '的', '作用', '，', '既', '给', '读者', '以', '美的', '鉴赏', '和', '启发', '，', '又', '引发', '了', '读者', '阅读', '的', '兴趣', '和', '购买', '的', '动机', '。', '这种', '社会', '价值', '超出', '了', '装帧', '设计', '艺术', '\\n', '价值', '本身', '的', '范围', '，', '而', '对', '整个', '图书市场', '起着', '不可', '忽视', '的', '调摄', '作用', '。', '\\n', ' ', ' ', ' ', ' ', '当今世界', '，', '在', '图书', '出版', '领域', '，', '已', '形成', '三种', '以书装', '艺术风格', '来', '促销', '的', '流派', '：', '英国', '以', '庄重', '、', '豪华', '、', '大方', '为', '特征', '；', '日本', '为首', '的', '东方文化', '风格', '，', '以', '和', '\\n', '谐', '、', '含蕴', '、', '抒情', '见长', '；', '美国', '的', '现代派', '风格', '，', '以', '感官', '刺激', '为', '特征', '。', '这', '三者', '在', '图书', '营销', '上', '各有', '成效', '，', '在', '读者', '圈内', '有着', '广泛', '而', '深远', '的', '影响', '。', '哪', '一类', '图书', '\\n', '应该', '采取', '何种', '风格', '，', '所谓', '“', '量体裁衣', '”', '，', '因书', '制宜', '，', '是', '编辑', '工作者', '所', '应', '考虑', '的', '，', '也', '是', '书评', '工作者', '进行', '评论', '的', '依据', '。', '\\n', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', '（', '本文', '责任编辑', ' ', ' ', '韩', '忠良', '）', '＊', '\\n', ' ', ' ', ' ', ' ', '\\n', ' ', ' ', ' ', ' ', '\\n', ' ', ' ', ' ', ' ', '\\n', ' ', '\\n']\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "def tokenize_text(text):\n",
    "    return list(jieba.cut(text))\n",
    "\n",
    "# 例如，分词某篇文章\n",
    "text = train_data[\"C3-Art\"][0]  # 假设是C3类别的第一篇文章\n",
    "tokens = tokenize_text(text)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "227ab0af-6bd1-4a17-8744-9decc19ec42d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "类别: C11-Space, 分词后的文章数: 30\n",
      "类别 C11-Space 第一篇文章分词结果: ['宇航学', '报', '\\n', 'JOURNAL', ' ', 'OF', ' ', 'ASTRONAUTICS', ' ', '\\n']...\n",
      "类别: C15-Energy, 分词后的文章数: 30\n",
      "类别 C15-Energy 第一篇文章分词结果: ['【', ' ', '日', ' ', ' ', '期', ' ', '】', '19961230', '\\n']...\n",
      "类别: C16-Electronics, 分词后的文章数: 27\n",
      "类别 C16-Electronics 第一篇文章分词结果: ['【', ' ', '日', ' ', ' ', '期', ' ', '】', '19960827', '\\n']...\n",
      "类别: C17-Communication, 分词后的文章数: 25\n",
      "类别 C17-Communication 第一篇文章分词结果: ['【', ' ', '日', ' ', ' ', '期', ' ', '】', '19961225', '\\n']...\n",
      "类别: C19-Computer, 分词后的文章数: 30\n",
      "类别 C19-Computer 第一篇文章分词结果: ['计算机', '应用', '\\n', 'COMPUTER', ' ', 'APPLICATIONS', '\\n', '1999', '年', ' ']...\n",
      "类别: C23-Mine, 分词后的文章数: 30\n",
      "类别 C23-Mine 第一篇文章分词结果: ['【', ' ', '日', ' ', ' ', '期', ' ', '】', '19960404', '\\n']...\n",
      "类别: C29-Transport, 分词后的文章数: 30\n",
      "类别 C29-Transport 第一篇文章分词结果: ['【', ' ', '日', ' ', ' ', '期', ' ', '】', '19960104', '\\n']...\n",
      "类别: C3-Art, 分词后的文章数: 30\n",
      "类别 C3-Art 第一篇文章分词结果: ['【', ' ', '文献号', ' ', '】', '1', '-', '2340', '\\n', '【']...\n",
      "类别: C31-Enviornment, 分词后的文章数: 30\n",
      "类别 C31-Enviornment 第一篇文章分词结果: ['产业', '与', '环境', '\\n', 'INDUSTRY', ' ', 'AND', ' ', 'ENVIRONMENT', '\\n']...\n",
      "类别: C32-Agriculture, 分词后的文章数: 30\n",
      "类别 C32-Agriculture 第一篇文章分词结果: ['【', ' ', '文献号', ' ', '】', '1', '-', '3012', '\\n', '【']...\n",
      "类别: C34-Economy, 分词后的文章数: 30\n",
      "类别 C34-Economy 第一篇文章分词结果: ['【', ' ', '文献号', ' ', '】', '1', '-', '80', '\\n', '【']...\n",
      "类别: C35-Law, 分词后的文章数: 30\n",
      "类别 C35-Law 第一篇文章分词结果: [' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ']...\n",
      "类别: C36-Medical, 分词后的文章数: 30\n",
      "类别 C36-Medical 第一篇文章分词结果: [' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ']...\n",
      "类别: C37-Military, 分词后的文章数: 30\n",
      "类别 C37-Military 第一篇文章分词结果: ['发信人', ':', ' ', 'snake', ' ', '(', '小龙', ')', ',', ' ']...\n",
      "类别: C38-Politics, 分词后的文章数: 30\n",
      "类别 C38-Politics 第一篇文章分词结果: ['【', ' ', '文献号', ' ', '】', '1', '-', '1415', '\\n', '【']...\n",
      "类别: C39-Sports, 分词后的文章数: 30\n",
      "类别 C39-Sports 第一篇文章分词结果: ['浙江', '体育', '科学', '\\n', 'ZHEJIANG', ' ', 'SPORTS', ' ', 'SCIENCE', '\\n']...\n",
      "类别: C4-Literature, 分词后的文章数: 30\n",
      "类别 C4-Literature 第一篇文章分词结果: ['【', ' ', '日', ' ', ' ', '期', ' ', '】', '19960420', '\\n']...\n",
      "类别: C5-Education, 分词后的文章数: 30\n",
      "类别 C5-Education 第一篇文章分词结果: ['【', ' ', '日', ' ', ' ', '期', ' ', '】', '19960526', '\\n']...\n",
      "类别: C6-Philosophy, 分词后的文章数: 30\n",
      "类别 C6-Philosophy 第一篇文章分词结果: ['发信人', ':', ' ', 'id', ' ', '(', '无忧', ')', ',', ' ']...\n",
      "类别: C7-History, 分词后的文章数: 30\n",
      "类别 C7-History 第一篇文章分词结果: ['【', ' ', '文献号', ' ', '】', '1', '-', '1338', '\\n', '【']...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import jieba\n",
    "\n",
    "def tokenize_text(text):\n",
    "    \"\"\"\n",
    "    使用jieba进行中文分词，返回分词后的词语列表\n",
    "    \"\"\"\n",
    "    return list(jieba.cut(text))\n",
    "\n",
    "def load_data_from_directory(data_dir, num_files=30):\n",
    "    \"\"\"\n",
    "    从每个类别文件夹中加载前 `num_files` 篇文章，并进行分词\n",
    "    \"\"\"\n",
    "    category_files = {}\n",
    "    \n",
    "    # 遍历每个类别文件夹\n",
    "    for category in os.listdir(data_dir):\n",
    "        category_path = os.path.join(data_dir, category)\n",
    "        \n",
    "        if os.path.isdir(category_path):  # 确保是文件夹\n",
    "            # 获取该类别文件夹下的所有txt文件\n",
    "            files = [f for f in os.listdir(category_path) if f.endswith('.txt')]\n",
    "            \n",
    "            # 选择前num_files篇文章\n",
    "            selected_files = files[:num_files]  # 选择前30篇文件\n",
    "            \n",
    "            category_files[category] = []\n",
    "            for file in selected_files:\n",
    "                file_path = os.path.join(category_path, file)\n",
    "                \n",
    "                # 读取文件内容，使用GBK编码并忽略解码错误\n",
    "                try:\n",
    "                    with open(file_path, 'r', encoding='gbk', errors='ignore') as f:\n",
    "                        content = f.read()\n",
    "                except Exception as e:\n",
    "                    print(f\"读取文件 {file_path} 时出错: {e}\")\n",
    "                    continue  # 如果出错，跳过该文件\n",
    "                \n",
    "                # 对每篇文章进行分词\n",
    "                tokenized_content = tokenize_text(content)\n",
    "                \n",
    "                # 将分词后的内容存储到字典中\n",
    "                category_files[category].append(tokenized_content)\n",
    "    \n",
    "    return category_files\n",
    "\n",
    "# 使用函数加载数据并对每篇文章进行分词\n",
    "train_data_dir = \"D:\\\\nlp文档\\\\fudan\\\\train\\\\train\"  # 替换为你的训练集路径\n",
    "train_data = load_data_from_directory(train_data_dir, num_files=30)\n",
    "\n",
    "# 打印每个类别的前30篇文章的分词结果数量\n",
    "for category, tokenized_texts in train_data.items():\n",
    "    print(f\"类别: {category}, 分词后的文章数: {len(tokenized_texts)}\")\n",
    "    # 如果你想查看具体的分词内容，可以打印第一篇文章的分词结果\n",
    "    print(f\"类别 {category} 第一篇文章分词结果: {tokenized_texts[0][:10]}...\")  # 只打印前10个词语\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d01b23a-69b0-4c49-b806-ca53b55ac5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# 加载BERT分词器\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "\n",
    "def encode_texts(texts, max_length=512):\n",
    "    \"\"\"\n",
    "    将分词后的文本转换为BERT可接受的输入格式\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "    return inputs\n",
    "\n",
    "# 准备训练数据\n",
    "texts = []\n",
    "labels = []\n",
    "\n",
    "# 假设你的分类标签是类别文件夹的名字\n",
    "category_to_id = {category: idx for idx, category in enumerate(train_data.keys())}\n",
    "\n",
    "for category, tokenized_texts in train_data.items():\n",
    "    label = category_to_id[category]\n",
    "    for tokens in tokenized_texts:\n",
    "        texts.append(\" \".join(tokens))  # 将分词后的文本拼接成一个字符串\n",
    "        labels.append(label)\n",
    "\n",
    "# 将文本转换为BERT模型所需的格式\n",
    "inputs = encode_texts(texts)\n",
    "\n",
    "# 转换标签为Tensor\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "# 创建训练数据集\n",
    "dataset = TensorDataset(inputs['input_ids'], inputs['attention_mask'], labels)\n",
    "\n",
    "# 创建DataLoader\n",
    "train_dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3548aa5c-b18d-4001-905b-ab4faf736026",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "D:\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=20, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW\n",
    "\n",
    "# 加载BERT模型，设置分类的类别数\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-chinese', num_labels=len(category_to_id))\n",
    "\n",
    "# 设置优化器\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# 将模型迁移到GPU（如果可用）\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b9de3cd-9941-48af-923f-c1c3c9fba32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/3:   0%|                                                                       | 0/74 [00:00<?, ?it/s]D:\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:439: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "Training Epoch 1/3: 100%|██████████████████████████████████████████████████████████████| 74/74 [00:38<00:00,  1.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 2.5817011626991064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/3: 100%|██████████████████████████████████████████████████████████████| 74/74 [00:37<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Loss: 0.9674247809358545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/3: 100%|██████████████████████████████████████████████████████████████| 74/74 [00:37<00:00,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Loss: 0.3565860614180565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# 训练模型\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_dataloader, desc=f\"Training Epoch {epoch + 1}/{epochs}\"):\n",
    "        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 前向传播\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # 反向传播和优化\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch + 1} - Loss: {avg_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dcbba59e-c41b-44e9-b5b4-f6d5e93e1aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "类别: C11-Space, 分词后的文章数: 10\n",
      "类别: C15-Energy, 分词后的文章数: 10\n",
      "类别: C16-Electronics, 分词后的文章数: 10\n",
      "类别: C17-Communication, 分词后的文章数: 10\n",
      "类别: C19-Computer, 分词后的文章数: 10\n",
      "类别: C23-Mine, 分词后的文章数: 10\n",
      "类别: C29-Transport, 分词后的文章数: 10\n",
      "类别: C3-Art, 分词后的文章数: 10\n",
      "类别: C31-Enviornment, 分词后的文章数: 10\n",
      "类别: C32-Agriculture, 分词后的文章数: 10\n",
      "类别: C34-Economy, 分词后的文章数: 10\n",
      "类别: C35-Law, 分词后的文章数: 10\n",
      "类别: C36-Medical, 分词后的文章数: 10\n",
      "类别: C37-Military, 分词后的文章数: 10\n",
      "类别: C38-Politics, 分词后的文章数: 10\n",
      "类别: C39-Sports, 分词后的文章数: 10\n",
      "类别: C4-Literature, 分词后的文章数: 10\n",
      "类别: C5-Education, 分词后的文章数: 10\n",
      "类别: C6-Philosophy, 分词后的文章数: 10\n",
      "类别: C7-History, 分词后的文章数: 10\n"
     ]
    }
   ],
   "source": [
    "# 使用之前的代码加载测试集并进行分词\n",
    "def load_test_data_from_directory(data_dir, num_files=10):\n",
    "    \"\"\"\n",
    "    从每个类别文件夹中加载前 `num_files` 篇文章，并进行分词\n",
    "    \"\"\"\n",
    "    category_files = {}\n",
    "\n",
    "    # 遍历每个类别文件夹\n",
    "    for category in os.listdir(data_dir):\n",
    "        category_path = os.path.join(data_dir, category)\n",
    "        \n",
    "        if os.path.isdir(category_path):  # 确保是文件夹\n",
    "            # 获取该类别文件夹下的所有txt文件\n",
    "            files = [f for f in os.listdir(category_path) if f.endswith('.txt')]\n",
    "            \n",
    "            # 选择前num_files篇文章\n",
    "            selected_files = files[:num_files]  # 选择前10篇文件\n",
    "            \n",
    "            category_files[category] = []\n",
    "            for file in selected_files:\n",
    "                file_path = os.path.join(category_path, file)\n",
    "                \n",
    "                # 读取文件内容，使用GBK编码并忽略解码错误\n",
    "                try:\n",
    "                    with open(file_path, 'r', encoding='gbk', errors='ignore') as f:\n",
    "                        content = f.read()\n",
    "                except Exception as e:\n",
    "                    print(f\"读取文件 {file_path} 时出错: {e}\")\n",
    "                    continue  # 如果出错，跳过该文件\n",
    "                \n",
    "                # 对每篇文章进行分词\n",
    "                tokenized_content = tokenize_text(content)\n",
    "                \n",
    "                # 将分词后的内容存储到字典中\n",
    "                category_files[category].append(tokenized_content)\n",
    "    \n",
    "    return category_files\n",
    "\n",
    "# 使用函数加载数据\n",
    "test_data_dir = \"D:\\\\nlp文档\\\\fudan\\\\test\\\\test\"  # 替换为你的测试集路径\n",
    "test_data = load_test_data_from_directory(test_data_dir, num_files=10)\n",
    "\n",
    "# 打印每个类别的前10篇文章的分词结果数量\n",
    "for category, tokenized_texts in test_data.items():\n",
    "    print(f\"类别: {category}, 分词后的文章数: {len(tokenized_texts)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e610255b-9ea8-4275-92e6-32eff9f17cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备测试数据\n",
    "test_texts = []\n",
    "test_labels = []\n",
    "\n",
    "# 将每个类别的分词结果及其标签整合\n",
    "for category, tokenized_texts in test_data.items():\n",
    "    label = category_to_id[category]  # 获取该类别的标签\n",
    "    for tokens in tokenized_texts:\n",
    "        test_texts.append(\" \".join(tokens))  # 将分词后的文本拼接成一个字符串\n",
    "        test_labels.append(label)  # 将该标签添加到labels列表\n",
    "\n",
    "# 将文本转换为BERT模型所需的格式\n",
    "test_inputs = encode_texts(test_texts)\n",
    "\n",
    "# 转换标签为Tensor\n",
    "test_labels = torch.tensor(test_labels)\n",
    "\n",
    "# 创建测试数据集\n",
    "test_dataset = TensorDataset(test_inputs['input_ids'], test_inputs['attention_mask'], test_labels)\n",
    "\n",
    "# 创建DataLoader\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5de3ac35-0fdd-4b23-97ca-a91975929a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9\n"
     ]
    }
   ],
   "source": [
    "# 使用与训练时相同的evaluate函数来评估模型\n",
    "test_accuracy = evaluate(model, test_dataloader)\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851d8dc8-24b9-4d61-83c8-0df6ce7d5382",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "\n",
    "# 假设你已经加载了训练好的BERT模型和tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "model.eval()  # 将模型设置为评估模式\n",
    "\n",
    "def predict_category(input_text, tokenizer, model):\n",
    "    # 将输入文本进行分词并转换为BERT输入格式\n",
    "    inputs = tokenizer(input_text, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    \n",
    "    # 将数据移到GPU（如果可用）\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "    model.to(device)\n",
    "    \n",
    "    # 进行推理\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    # 获取预测的标签（类别）\n",
    "    predicted_label = torch.argmax(logits, dim=1).item()\n",
    "    return predicted_label\n",
    "\n",
    "# 假设你有一篇文章\n",
    "input_text = \"这里是你想分类的文章内容\"\n",
    "\n",
    "# 预测文章的类别\n",
    "predicted_label = predict_category(input_text, tokenizer, model)\n",
    "\n",
    "# 获取类别名称（可以根据之前创建的 category_to_id 字典进行反向查找）\n",
    "id_to_category = {idx: category for category, idx in category_to_id.items()}\n",
    "predicted_category = id_to_category[predicted_label]\n",
    "\n",
    "print(f\"预测的类别是: {predicted_category}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
